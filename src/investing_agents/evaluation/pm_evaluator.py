"""Senior Buy-Side Portfolio Manager Report Evaluator.

This module automatically evaluates investment reports from the perspective
of an experienced buy-side PM, providing grades and actionable feedback.
"""

from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional

import structlog
from claude_agent_sdk import AssistantMessage, ClaudeAgentOptions, TextBlock, query

logger = structlog.get_logger(__name__)


class PMEvaluator:
    """Evaluates investment reports as a senior buy-side portfolio manager.

    Provides detailed assessment, grading, and improvement suggestions
    for generated investment reports.
    """

    def __init__(self):
        """Initialize PM evaluator."""
        self.log = logger

    async def evaluate_report(
        self,
        report: Dict[str, Any],
        valuation: Optional[Dict[str, Any]] = None,
        ticker: str = "",
        html_content: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Evaluate investment report from PM perspective.

        Args:
            report: Final narrative report JSON
            valuation: DCF valuation summary
            ticker: Stock ticker
            html_content: Generated HTML report (for presentation checks)

        Returns:
            Dict with:
                - overall_grade: Letter grade (A+, A, A-, B+, B, B-, etc.)
                - overall_score: Numeric score (0-100)
                - dimension_scores: Scores for each evaluation dimension
                - strengths: List of what works well
                - critical_issues: Must-fix issues
                - improvements: Suggested improvements
                - actionable_next_steps: Specific actions to improve
                - evaluation_summary: Text summary of evaluation
        """
        self.log.info("pm_evaluation.start", ticker=ticker)

        # Build evaluation prompt
        prompt = self._build_evaluation_prompt(report, valuation, ticker, html_content)

        # Get evaluation from Claude (acting as senior PM)
        options = ClaudeAgentOptions(
            system_prompt=self._get_system_prompt(),
            max_turns=1,  # Single evaluation turn
        )

        # Collect text from AssistantMessage
        full_response = ""
        async for message in query(prompt=prompt, options=options):
            if isinstance(message, AssistantMessage):
                for block in message.content:
                    if isinstance(block, TextBlock):
                        full_response += block.text

        # Save raw response for debugging (useful when parsing fails)
        self.log.debug(
            "pm_evaluation.raw_response_received",
            response_length=len(full_response),
            has_json_markers='{' in full_response and '}' in full_response
        )

        # DEBUG: Save raw response to file for inspection
        import tempfile
        debug_file = Path(tempfile.gettempdir()) / "pm_eval_raw_response.txt"
        with open(debug_file, "w") as f:
            f.write(f"===== PM EVALUATOR RAW RESPONSE ({ticker}) =====\n")
            f.write(f"Length: {len(full_response)} characters\n")
            f.write(f"Has braces: {'{' in full_response and '}' in full_response}\n")
            f.write("\n===== RESPONSE CONTENT =====\n")
            f.write(full_response)
        self.log.info("pm_evaluation.raw_response_saved", debug_file=str(debug_file))

        # Parse response
        evaluation = self._parse_evaluation_response(full_response)

        self.log.info(
            "pm_evaluation.complete",
            ticker=ticker,
            grade=evaluation.get("overall_grade"),
            score=evaluation.get("overall_score"),
        )

        return evaluation

    def _get_system_prompt(self) -> str:
        """Get system prompt for PM evaluation."""
        return """You are a senior buy-side portfolio manager with 20+ years of experience at top hedge funds (Citadel, Bridgewater, Tiger Global).

Your expertise includes:
- Fundamental equity analysis across all sectors
- DCF valuation and scenario modeling
- Risk assessment and portfolio construction
- Reading 100+ equity research reports per week
- Making multi-million dollar investment decisions

You are reviewing an investment report generated by a junior analyst. You have been provided with a summary of the report content - evaluate based on what is provided, do NOT request additional information.

Your task is to evaluate the report critically and provide:
1. A letter grade (A+, A, A-, B+, B, B-, C+, C, C-, D, F)
2. A numeric score (0-100)
3. Detailed feedback on strengths and weaknesses
4. Specific, actionable suggestions for improvement

Be honest, direct, and constructive. Your feedback should help the analyst write better reports.

EVALUATION CRITERIA:
1. **Decision-Readiness (25 points)**: Can I make a BUY/HOLD/SELL decision in 10 minutes?
2. **Data Quality (20 points)**: Are claims backed by evidence? Are numbers accurate?
3. **Investment Thesis (20 points)**: Is the thesis clear, specific, and differentiated?
4. **Financial Analysis (15 points)**: Are financials comprehensive and insightful?
5. **Risk Assessment (10 points)**: Are risks material, specific, and well-mitigated?
6. **Presentation (10 points)**: Is the report scannable, visual, and well-structured?

GRADING SCALE:
- A+ (97-100): Exceptional, institutional-grade, ready for IC presentation
- A (93-96): Excellent, minor polish needed
- A- (90-92): Very good, a few improvements needed
- B+ (87-89): Good, some gaps to address
- B (83-86): Adequate, multiple improvements needed
- B- (80-82): Below expectations, significant work required
- C+ to F (<80): Major revisions required

**AUTOMATIC GRADE CAPS FOR CRITICAL ERRORS:**
If ANY of these fundamental errors exist, maximum grade is capped:
- Mathematical errors in DCF valuation (wrong shares outstanding, market cap confusion): **MAX GRADE = C (75)**
- Internal inconsistency (valuation vs recommendation mismatch >100% upside): **MAX GRADE = C+ (78)**
- Missing required components (scenarios, financial model, competitive analysis): **MAX GRADE = B- (82)**
- Data fabrication or unsourced claims: **MAX GRADE = D (65)**

Apply the LOWEST cap if multiple errors exist. For example, if report has both DCF math error AND missing scenarios, max grade = C (75).

Respond ONLY in valid JSON format with this structure:
{
  "overall_grade": "A-",
  "overall_score": 91,
  "dimension_scores": {
    "decision_readiness": 23,
    "data_quality": 18,
    "investment_thesis": 19,
    "financial_analysis": 14,
    "risk_assessment": 9,
    "presentation": 8
  },
  "strengths": ["Strength 1", "Strength 2", ...],
  "critical_issues": ["Issue 1", "Issue 2", ...],
  "improvements": ["Improvement 1", "Improvement 2", ...],
  "actionable_next_steps": ["Step 1", "Step 2", ...],
  "evaluation_summary": "2-3 paragraph summary of overall assessment"
}
"""

    def _build_evaluation_prompt(
        self,
        report: Dict[str, Any],
        valuation: Optional[Dict[str, Any]],
        ticker: str,
        html_content: Optional[str],
    ) -> str:
        """Build evaluation prompt with report content."""
        # Extract key sections
        exec_summary = report.get("executive_summary", {})
        thesis = report.get("investment_thesis", {})
        financial = report.get("financial_analysis", {})
        risks = report.get("risks", {})
        recommendation = report.get("recommendation", {})

        # Count data availability
        has_valuation = valuation is not None
        has_current_price = valuation.get("current_price") if valuation else None
        has_scenarios = bool(report.get("valuation", {}).get("scenarios"))

        # Build prompt with COMPLETE content (no truncation)
        prompt = f"""EVALUATE THIS INVESTMENT REPORT

**Ticker:** {ticker}

**REPORT CONTENT:**

**Executive Summary:**
{exec_summary.get('thesis', 'MISSING')}

**Catalysts ({len(exec_summary.get('catalysts', []))} listed):**
{chr(10).join(f"- {c}" for c in exec_summary.get('catalysts', [])[:5])}

**Key Risks ({len(exec_summary.get('risks', []))} listed):**
{chr(10).join(f"- {r}" for r in exec_summary.get('risks', [])[:5])}

**Investment Thesis:**
{thesis.get('core_hypothesis', 'MISSING')}

**Financial Analysis:**
- Revenue analysis: {'Present' if financial.get('revenue_drivers') or financial.get('revenue_analysis') else 'MISSING'}
- Margin analysis: {'Present' if financial.get('margin_dynamics') or financial.get('margin_analysis') else 'MISSING'}
- Cash flow analysis: {'Present' if financial.get('cash_flow') or financial.get('cash_flow_analysis') else 'MISSING'}

**Valuation:**
- DCF valuation: {'Present' if has_valuation else 'MISSING'}
- Current price: {f'${has_current_price:.2f}' if has_current_price else 'MISSING'}
- Fair value: {f"${valuation.get('fair_value_per_share', 0):.2f}" if valuation else 'MISSING'}
- Scenarios: {'Present (Bull/Base/Bear)' if has_scenarios else 'MISSING'}

**Risk Assessment:**
- Total risks identified: {sum(len(v) for v in risks.values() if isinstance(v, list))}
- Risk categories: {len([k for k, v in risks.items() if isinstance(v, list) and v])}

**Recommendation:**
- Action: {recommendation.get('action', 'MISSING')}
- Conviction: {recommendation.get('conviction', 'MISSING')}
- Timeframe: {recommendation.get('timeframe', 'MISSING')}
- Entry conditions: {len(recommendation.get('entry_conditions', []))}
- Exit conditions: {len(recommendation.get('exit_conditions', []))}

**PRESENTATION (HTML Report):**
{self._assess_html_presentation(html_content) if html_content else 'HTML content not provided for evaluation'}

---

**YOUR TASK:**

Evaluate this report as if you're deciding whether to present it to your Investment Committee.

Ask yourself:
1. Can I make a BUY/HOLD/SELL decision in 10 minutes with this report?
2. Are the key claims backed by specific evidence and numbers?
3. Is the investment thesis differentiated and compelling?
4. Are the financials detailed enough to understand the business?
5. Are the risks material and well-thought-out?
6. Is the report scannable and well-presented?

Provide honest, specific feedback in JSON format as specified in your system prompt.
"""
        return prompt

    def _assess_html_presentation(self, html_content: str) -> str:
        """Quick assessment of HTML presentation quality."""
        checks = []

        # Check for key features
        if "current-price" in html_content or "Current Price" in html_content:
            checks.append("✓ Stock price visible in header")
        else:
            checks.append("✗ Stock price NOT visible")

        if "investment-snapshot" in html_content or "Investment Snapshot" in html_content:
            checks.append("✓ Investment snapshot table present")
        else:
            checks.append("✗ No investment snapshot table")

        if "projections-table" in html_content or "projections-section" in html_content:
            checks.append("✓ Financial projections table present")
        else:
            checks.append("✗ No financial projections table")

        if "scenarios-table" in html_content or "scenarios-section" in html_content:
            checks.append("✓ Valuation scenarios table present")
        else:
            checks.append("✗ No scenarios table")

        if "risk-details" in html_content or "<details" in html_content:
            checks.append("✓ Collapsible sections present")
        else:
            checks.append("✗ No collapsible sections")

        return "\n".join(checks)

    def _parse_evaluation_response(self, response: str) -> Dict[str, Any]:
        """Parse evaluation response from Claude."""
        import json
        import re

        # Log raw response for debugging (first 500 chars)
        self.log.debug(
            "pm_evaluation.parsing_response",
            response_preview=response[:500] if response else "EMPTY"
        )

        # Strategy 1: Try to extract JSON from markdown code blocks
        code_block_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
        if code_block_match:
            try:
                evaluation = json.loads(code_block_match.group(1))
                self.log.info("pm_evaluation.parsed_from_code_block")
                return evaluation
            except json.JSONDecodeError as e:
                self.log.warning("pm_evaluation.code_block_parse_failed", error=str(e))

        # Strategy 2: Try to find JSON object (balanced braces)
        # Find the first '{' and then match balanced braces
        try:
            start_idx = response.find('{')
            if start_idx != -1:
                # Find matching closing brace
                brace_count = 0
                end_idx = start_idx
                for i in range(start_idx, len(response)):
                    if response[i] == '{':
                        brace_count += 1
                    elif response[i] == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            end_idx = i + 1
                            break

                if end_idx > start_idx:
                    json_str = response[start_idx:end_idx]
                    evaluation = json.loads(json_str)
                    self.log.info("pm_evaluation.parsed_from_balanced_braces")
                    return evaluation
        except (json.JSONDecodeError, ValueError) as e:
            self.log.warning("pm_evaluation.balanced_brace_parse_failed", error=str(e))

        # Strategy 3: Try the original greedy regex as last resort
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            try:
                evaluation = json.loads(json_match.group(0))
                self.log.info("pm_evaluation.parsed_from_greedy_regex")
                return evaluation
            except json.JSONDecodeError as e:
                self.log.warning("pm_evaluation.greedy_regex_parse_failed", error=str(e))

        # Fallback: return default structure with more diagnostic info
        self.log.error(
            "pm_evaluation.all_parsing_strategies_failed",
            response_length=len(response),
            has_opening_brace='{' in response,
            has_closing_brace='}' in response,
        )

        return {
            "overall_grade": "N/A",
            "overall_score": 0,
            "dimension_scores": {},
            "strengths": [],
            "critical_issues": ["Failed to parse evaluation response"],
            "improvements": [],
            "actionable_next_steps": [],
            "evaluation_summary": "Evaluation parsing failed. Please review manually.",
        }

    def save_evaluation(
        self,
        evaluation: Dict[str, Any],
        output_dir: Path,
        ticker: str,
    ) -> Path:
        """Save evaluation to file.

        Args:
            evaluation: Evaluation results
            output_dir: Directory to save evaluation
            ticker: Stock ticker

        Returns:
            Path to saved evaluation file
        """
        # Create evaluation directory if needed
        eval_dir = output_dir / "evaluation"
        eval_dir.mkdir(parents=True, exist_ok=True)

        # Save JSON evaluation
        json_path = eval_dir / "pm_evaluation.json"
        import json
        with open(json_path, "w") as f:
            json.dump(evaluation, f, indent=2)

        # Also create a human-readable markdown report
        md_path = eval_dir / "pm_evaluation.md"
        md_content = self._format_evaluation_markdown(evaluation, ticker)
        with open(md_path, "w") as f:
            f.write(md_content)

        self.log.info(
            "pm_evaluation.saved",
            json_path=str(json_path),
            md_path=str(md_path),
        )

        return md_path

    def _format_evaluation_markdown(
        self,
        evaluation: Dict[str, Any],
        ticker: str,
    ) -> str:
        """Format evaluation as readable markdown."""
        grade = evaluation.get("overall_grade", "N/A")
        score = evaluation.get("overall_score", 0)
        dim_scores = evaluation.get("dimension_scores", {})

        md = f"""# Portfolio Manager Evaluation: {ticker}

**Date:** {datetime.now().strftime("%B %d, %Y at %H:%M")}

---

## Overall Assessment

**Grade:** {grade}
**Score:** {score}/100

---

## Dimension Scores

| Dimension | Score | Max | Percentage |
|-----------|-------|-----|------------|
| Decision-Readiness | {dim_scores.get('decision_readiness', 0)} | 25 | {(dim_scores.get('decision_readiness', 0)/25*100):.0f}% |
| Data Quality | {dim_scores.get('data_quality', 0)} | 20 | {(dim_scores.get('data_quality', 0)/20*100):.0f}% |
| Investment Thesis | {dim_scores.get('investment_thesis', 0)} | 20 | {(dim_scores.get('investment_thesis', 0)/20*100):.0f}% |
| Financial Analysis | {dim_scores.get('financial_analysis', 0)} | 15 | {(dim_scores.get('financial_analysis', 0)/15*100):.0f}% |
| Risk Assessment | {dim_scores.get('risk_assessment', 0)} | 10 | {(dim_scores.get('risk_assessment', 0)/10*100):.0f}% |
| Presentation | {dim_scores.get('presentation', 0)} | 10 | {(dim_scores.get('presentation', 0)/10*100):.0f}% |

---

## Summary

{evaluation.get('evaluation_summary', 'No summary provided.')}

---

## Strengths

"""
        for strength in evaluation.get("strengths", []):
            md += f"- ✅ {strength}\n"

        md += "\n---\n\n## Critical Issues\n\n"
        for issue in evaluation.get("critical_issues", []):
            md += f"- ⚠️ {issue}\n"

        md += "\n---\n\n## Suggested Improvements\n\n"
        for improvement in evaluation.get("improvements", []):
            md += f"- 💡 {improvement}\n"

        md += "\n---\n\n## Actionable Next Steps\n\n"
        for i, step in enumerate(evaluation.get("actionable_next_steps", []), 1):
            md += f"{i}. {step}\n"

        md += "\n---\n\n*This evaluation was automatically generated by the PM Evaluator system.*\n"

        return md
